{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee4a0b5",
   "metadata": {},
   "source": [
    "# üß™ Consigna de Evaluaci√≥n Parcial - Procesamiento de Texto y Clustering\n",
    "\n",
    "## üéØ Objetivo\n",
    "#### Aplicar t√©cnicas de procesamiento de texto y representaci√≥n vectorial para agrupar comentarios en espa√±ol en funci√≥n de su contenido sem√°ntico, utilizando t√©cnicas de clustering no supervisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0304e135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Importaciones necesarias\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7591bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lolonastri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lolonastri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lolonastri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Procesamiento\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b93f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de stopwords en ingl√©s\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55a659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset IMDb (en ingl√©s)\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f425b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos una muestra de 1000 textos\n",
    "df = pd.DataFrame(dataset['train']).sample(frac=0.02, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094a73db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos originales:\n",
      "1. Dumb is as dumb does, in this thoroughly uninteresting, supposed black comedy. Essentially what starts out as Chris Klein trying to maintain a low profile, eventually morphs into an uninspired version...\n",
      "\n",
      "2. I dug out from my garage some old musicals and this is another one of my favorites. It was written by Jay Alan Lerner and directed by Vincent Minelli. It won two Academy Awards for Best Picture of 195...\n",
      "\n",
      "3. After watching this movie I was honestly disappointed - not because of the actors, story or directing - I was disappointed by this film advertisements.<br /><br />The trailers were suggesting that the...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostramos los primeros textos originales\n",
    "print(\"Ejemplos originales:\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. {df['text'][i][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311afff1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0591ed",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento de texto\n",
    "\n",
    "Aplic√° las siguientes tareas sobre una columna de comentarios o textos en espa√±ol:\n",
    "\n",
    "- **Tokenizaci√≥n:** Convert√≠ cada texto en una secuencia de palabras.\n",
    "- **Eliminaci√≥n de stopwords:** Remov√© palabras vac√≠as utilizando una lista en espa√±ol.\n",
    "- **Stemming o Lematizaci√≥n:** Eleg√≠ una de las dos t√©cnicas y aplicala sobre los tokens procesados.\n",
    "\n",
    "üìå *Mostr√° ejemplos del texto antes y despu√©s del preprocesamiento.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3db28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Funci√≥n de preprocesamiento\n",
    "def preprocess_text(text):\n",
    "    tokens = text.lower().split()  # tokenizaci√≥n b√°sica\n",
    "    tokens = [t.strip(string.punctuation) for t in tokens if t.isalpha()]  # limpiar signos\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # quitar stopwords\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]  # lematizar\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "920b1f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Texto original #1:\n",
      "Dumb is as dumb does, in this thoroughly uninteresting, supposed black comedy. Essentially what starts out as Chris Klein trying to maintain a low profile, eventually morphs into an uninspired version of \"The Three Amigos\", only without any laughs. In order for black comedy to work, it must be outra...\n",
      "\n",
      "‚úÖ Tokens procesados #1:\n",
      "['dumb', 'dumb', 'thoroughly', 'supposed', 'black', 'essentially', 'start', 'chris', 'klein', 'trying', 'maintain', 'low', 'eventually', 'morphs', 'uninspired', 'version', 'three', 'without', 'order', 'black', 'comedy', 'must', 'order', 'black', 'comedy', 'cannot', 'mean', 'really', 'town', 'full']...\n",
      "\n",
      "\n",
      "üìÑ Texto original #2:\n",
      "I dug out from my garage some old musicals and this is another one of my favorites. It was written by Jay Alan Lerner and directed by Vincent Minelli. It won two Academy Awards for Best Picture of 1951 and Best Screenplay. The story of an American painter in Paris who tries to make it big. Nina Foch...\n",
      "\n",
      "‚úÖ Tokens procesados #2:\n",
      "['dug', 'garage', 'old', 'musical', 'another', 'one', 'written', 'jay', 'alan', 'lerner', 'directed', 'vincent', 'two', 'academy', 'award', 'best', 'picture', 'best', 'story', 'american', 'painter', 'paris', 'try', 'make', 'nina', 'foch', 'sophisticated', 'lady', 'mean', 'interested']...\n",
      "\n",
      "\n",
      "üìÑ Texto original #3:\n",
      "After watching this movie I was honestly disappointed - not because of the actors, story or directing - I was disappointed by this film advertisements.<br /><br />The trailers were suggesting that the battalion \"have chosen the third way out\" other than surrender or die (Polish infos were even misgu...\n",
      "\n",
      "‚úÖ Tokens procesados #3:\n",
      "['watching', 'movie', 'honestly', 'disappointed', 'story', 'directing', 'disappointed', 'film', 'trailer', 'suggesting', 'battalion', 'chosen', 'third', 'way', 'surrender', 'die', 'info', 'even', 'misguiding', 'choice', 'killed', 'artillery', 'german', 'even', 'translated', 'title', 'wrong', 'tickled', 'right', 'spot']...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Columna con tokens preprocesados\n",
    "df['tokens'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar ejemplos antes y despu√©s\n",
    "for i in range(3):\n",
    "    print(f\"\\nüìÑ Texto original #{i+1}:\\n{df['text'][i][:300]}...\\n\")\n",
    "    print(f\"‚úÖ Tokens procesados #{i+1}:\\n{df['tokens'][i][:30]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88010cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6da10",
   "metadata": {},
   "source": [
    "## 2. Representaci√≥n vectorial\n",
    "\n",
    "Convert√≠ los textos preprocesados en vectores num√©ricos utilizando alguna de las siguientes t√©cnicas:\n",
    "\n",
    "- Bag-of-Words (BoW)\n",
    "- TF-IDF (preferido si vas a comparar pesos entre palabras)\n",
    "\n",
    "üìå *Visualiz√° brevemente la matriz resultante y cont√° cu√°ntas dimensiones tiene.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abcc34",
   "metadata": {},
   "source": [
    "# 1) BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f11cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir los tokens preprocesados en un string nuevamente\n",
    "df['clean_text'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b53af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß± Matriz BoW: (500, 8993)\n",
      "üìÑ Ejemplo de palabras:\n",
      "['aamir' 'abandon' 'abandoned' 'abashed' 'abby' 'abc' 'abdominal'\n",
      " 'abducted' 'abducting' 'abetted' 'ability' 'abject' 'able' 'aboard'\n",
      " 'abominable' 'abomination' 'aboriginal' 'aborted' 'abound' 'abroad']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Crear vectorizador BoW\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "print(f\"üß± Matriz BoW: {X_bow.shape}\")\n",
    "print(f\"üìÑ Ejemplo de palabras:\\n{bow_vectorizer.get_feature_names_out()[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc0475",
   "metadata": {},
   "source": [
    "# 2) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0022dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Crear el vectorizador TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transformar el texto a vectores num√©ricos\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efb2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ La matriz TF-IDF tiene forma: (500, 8993)\n",
      "üß† N√∫mero de dimensiones (palabras √∫nicas): 8993\n"
     ]
    }
   ],
   "source": [
    "# Mostrar forma de la matriz y algunas palabras\n",
    "print(f\"‚úÖ La matriz TF-IDF tiene forma: {X_tfidf.shape}\")\n",
    "print(f\"üß† N√∫mero de dimensiones (palabras √∫nicas): {len(tfidf_vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1e77a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî§ Algunas palabras del vocabulario:\n",
      "['aamir' 'abandon' 'abandoned' 'abashed' 'abby' 'abc' 'abdominal'\n",
      " 'abducted' 'abducting' 'abetted' 'ability' 'abject' 'able' 'aboard'\n",
      " 'abominable' 'abomination' 'aboriginal' 'aborted' 'abound' 'abroad']\n"
     ]
    }
   ],
   "source": [
    "# Ver algunas features (palabras del vocabulario)\n",
    "print(f\"\\nüî§ Algunas palabras del vocabulario:\\n{tfidf_vectorizer.get_feature_names_out()[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae62d7b",
   "metadata": {},
   "source": [
    "# 3) Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8606473",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c41fc",
   "metadata": {},
   "source": [
    "## 3. Agrupamiento (Clustering)\n",
    "\n",
    "Aplic√° un algoritmo de clustering no supervisado para agrupar los comentarios:\n",
    "\n",
    "- Puede ser **KMeans**, **Agglomerative Clustering** o **DBSCAN**.\n",
    "- Determin√° el n√∫mero de clusters si es necesario (por ejemplo, usando el m√©todo del codo para KMeans).\n",
    "- Asign√° cada comentario a un grupo (cluster).\n",
    "\n",
    "üìå *Visualiz√° los clusters utilizando reducci√≥n de dimensionalidad (por ejemplo con PCA, t-SNE o UMAP).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e62691",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37182386",
   "metadata": {},
   "source": [
    "## 4. An√°lisis e interpretaci√≥n\n",
    "\n",
    "- Describ√≠ brevemente qu√© patrones observ√°s en cada cluster.\n",
    "- Mostr√° ejemplos representativos de cada grupo.\n",
    "- Discut√≠ si los clusters parecen alinearse con diferentes tonos, temas o polaridades de los textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38d1f9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326159a",
   "metadata": {},
   "source": [
    "## üí° Bonus (opcional)\n",
    "\n",
    "- Etiquet√° manualmente una muestra de los textos con polaridad (positivo/negativo) y analiz√° si los clusters coinciden con estas etiquetas.\n",
    "- Compar√° los resultados usando TF-IDF vs BoW."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "POO-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
